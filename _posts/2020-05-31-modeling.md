---
title: "What is Science?"
tags: [statistics, modeling, science, covid]
excerpt: "Do you trust the models?"
last_modified_at: 2021-07-04
category: [data science, thoughts]
header:
    overlay_image: '/assets/images/modeling/Screen Shot 2021-07-03 at 9.22.33 PM.png'
    overlay_filter: 0.5
---

Statistics is a hard subject. There are no right answers, there are only justified and unjustified claims with giant gray areas. Understanding statistics takes a sophisticated understanding of causality, logic and difficult math. And since there is no simple verification, statisticians have to be risk takers and humble at the same time. Unfortunately, a lot of scientists who rely on statistics don't have a very sophisticated grasp of the tools they're using. Some scientists don't undertsand the statistics they use at all. In fact, not even everyone who has studied statistics understands statistics!

This puts us, the public, in a difficult position as we try to understand the pandemic through mountains of unfiltered data. The task is monumental, and the room for error is enormous. This seems to have gotten to the staff at the New York Times, where they [claimed](https://www.nytimes.com/2020/05/23/reader-center/coronavirus-new-york-times-front-page.html) to publish a list of COVID-19 deaths as a result of "...a fatigue with the data."

People are declaring that they want their leaders to listen to science, calling Trump a [geocentrist](https://www.salon.com/2020/05/03/our-anti-science-leaders-are-the-geocentrists-of-today/) and [banning](https://www.bbc.com/news/technology-52388586) medical advice that goes against the WHO guidelines. 

Many scientists interviewed by the media claim to represent the views of the "scientific community", taking brave stands against ["bullshit"](https://www.theguardian.com/world/2020/apr/28/there-is-no-absolute-truth-an-infectious-disease-expert-on-covid-19-misinformation-and-bullshit).

But what does it mean to listen to the scientists? Does that mean that we should trust their models wihtout understanding them? Even Dr. Anthony Fauci warned us not to look to models like oracles. He seemed to be advocating for more nuance when he [said](https://www.washingtonpost.com/health/2020/04/02/experts-trumps-advisers-doubt-white-houses-240000-coronavirus-deaths-estimate/) “I’ve looked at all the models. I’ve spent a lot of time on the models. They don’t tell you anything. You can’t really rely upon models.”

I think it's helpful to explore what exactly a model is. Going to the [Wikipedia page](https://en.wikipedia.org/wiki/Scientific_modelling) for scientific modelling, I think we get a much more enlightening description of the relationship between science and modelling from John von Neumann:

> ... **the sciences do not try to explain, they hardly even try to interpret, they mainly make models.** By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work—that is, correctly to describe phenomena from a reasonably wide area.

**The key idea here is that a model is neither an explanation nor an interpretation.** It is an imperfect tool used to predict.

Here is how I would define a model: A model is an *assumption* about how the world works. We can use our model to *predict* things that have not been observed. And since no model is perfect, hopefully we can report how *confident* we are in our model's prediction.

I think it is helpful to work through an example. Let's suppose that we have a list of trees and we are interested in modelling the relationship between height and volume of the tree. Below is a table of the first 10 examples in the dataset:

|    |   Height (ft.) |   Volume (ft^3) |
|---:|---------:|---------:|
|  0 |       70 |     10.3 |
|  1 |       65 |     10.3 |
|  2 |       63 |     10.2 |
|  3 |       72 |     16.4 |
|  4 |       81 |     18.8 |
|  5 |       83 |     19.7 |
|  6 |       66 |     15.6 |
|  7 |       75 |     18.2 |
|  8 |       80 |     22.6 |
|  9 |       75 |     19.9 |

And here are all 31 examples in a scatter plot:

![Scatter plot](/assets/modeling/treeplot.jpg)

It looks kind of like we could draw  line to fit this data, perhaps something like this:

![assumption](/assets/modeling/assumption.jpg)

Okay, so let's start by making an assumption about the world and defining our model: I propose that the volume of a tree is equal to 2\*height \- 120 (that's the equation of the red line).

Now let's make a prediction. If we come across a new tree that has a height of 85 ft., then we can predict that the volume of the tree is 2\*85-120 = 50 ft. ^3.

How confident are we in this prediction? Well, obviously our model is not literally true, the points on the graph don't lie perfectly on the line. In fact, around the area where height = 85 ft., the points are quite far from the line! This makes me not so confident in my prediction. I'm going to say that I predict the volume to be (50 ± 20) ft.^3. The purple arrows below show this "confidence":

![confidence](/assets/modeling/confidence.jpg)

So we have defined our model and made use of it. We made an **assumption** about trees, and we used it to make a **prediction** about a certain tree's height, and report **confidence** in our prediction.

How do we ordinary people know if we should trust the model though? Well as a statistician, I can tell you everything you'll need to know. Here's a *serious* linear model with a bunch of relevant statistics and metrics. Instead of just drawing a line, the code below picked the *best* line using a method called Ordinary Least Squares (OLS).

```python
import pandas as pd
from matplotlib import pyplot as plt
import statsmodels.api as sm

trees = pd.read_csv('https://forge.scilab.org/index.php/p/rdataset/source/file/master/csv/datasets/trees.csv')

out = sm.OLS(trees['Height'],trees['Volume'])
result = out.fit()
print(result.summary())
```

```

                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                 Height   R-squared (uncentered):                   0.813
Model:                            OLS   Adj. R-squared (uncentered):              0.807
Method:                 Least Squares   F-statistic:                              130.4
Date:                Sun, 31 May 2020   Prob (F-statistic):                    1.91e-12
Time:                        16:12:06   Log-Likelihood:                         -152.36
No. Observations:                  31   AIC:                                      306.7
Df Residuals:                      30   BIC:                                      308.2
Df Model:                           1                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Volume         2.0086      0.176     11.418      0.000       1.649       2.368
==============================================================================
Omnibus:                        7.306   Durbin-Watson:                   0.120
Prob(Omnibus):                  0.026   Jarque-Bera (JB):                6.242
Skew:                          -1.089   Prob(JB):                       0.0441
Kurtosis:                       3.299   Cond. No.                         1.00
==============================================================================
>
Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```

But what does this all mean? Is this model better like I suggested? How do we know if we should trust it?

Well it's a good thing I know stats. From this I can see that our model has a BIC of 308.2 and a Durbin-Watson score of 0.120. These are very impressive numbers, which means that this is a very good model.

Okay, I made that all up. In fact, the BIC is meaningless on its own. It's meant to be compared to another BIC.

Why did I just make that up? I wanted to prove a point, that expertise and complicated numbers can obfuscate bad claims about the world.

There are obvious flaws with these models that take no special knowledge to understand. For example, we're only predicting volume from height. This means our models predict the same volume for skinny trees and fat trees. And it takes no education to realize that a tree isn't even cyllindrical. It's thickest at its base and tapers off towards the top. And for reasons I won't get into, it was actually [inppropriate](https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions) for me to fit an OLS model to this data. All those numbers I printed out above rest on false assumptions.

So anyone using their common sense could conclude that this model doesn't make sense. To reject a model is certainly *not* to reject science. And often rejecting a model doesn't  require scientific credentials, only common sense. Despite any inabillity to interpret or dispute the above printout, you can safely say that this model is a bad model for predicting tree volume.

This goes for the controversial [Imperial College study from March 2020](https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf) that predicted hospital overload in the US and UK. Reading the paper, we can see that a lot of their assumptions came from limited data and studies from China. It makes a lot of assumptions about where people go, live, interact, how often they interact, what the incubation period is, etc. It's an interesting exercise to see how far off some of these assumptions are, like their estimate of the infection fatality rate.

Common sense is essential in the sciences, and ordinary people have the power to use their common sense effectively. Faith in science seems to rest on the idea that the sciences are complex and require high IQs or years of education to grapple with. This is obviously somewhat true, but often people underestimate how simple a lot of the claims and assumptions are. 